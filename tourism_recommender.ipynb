{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diangelion/tourism-recommender/blob/main/tourism_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLfAa9NFHMQ6"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAj5OOHVHMBl"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from deep_translator import GoogleTranslator\n",
        "\n",
        "# # Inisialisasi translator\n",
        "# translator = GoogleTranslator(source='indonesian', target='english')\n",
        "\n",
        "# # Membaca file paragraf\n",
        "# paragraphs = pd.read_csv(\"/content/drive/MyDrive/LLM/Rekomendasi Tempat Wisata di Indonesia/Dataset/paragraphs_id.csv\")\n",
        "\n",
        "# # List untuk menyimpan hasil terjemahan\n",
        "# translated_list = []\n",
        "\n",
        "# # Iterasi dan terjemahkan setiap paragraf\n",
        "# for _, row in paragraphs.iterrows():\n",
        "#     paragraph = row[\"Paragraph\"]\n",
        "#     try:\n",
        "#         translated_paragraph = translator.translate(text=paragraph)\n",
        "#     except Exception as e:\n",
        "#         translated_paragraph = f\"[TRANSLATION ERROR] {e}\"\n",
        "#     translated_list.append(translated_paragraph)\n",
        "\n",
        "# # Buat DataFrame baru dari hasil terjemahan\n",
        "# translated_df = pd.DataFrame({'Paragraph': translated_list})\n",
        "\n",
        "# # Simpan ke file CSV\n",
        "# translated_df.to_csv(\"/content/drive/MyDrive/LLM/Rekomendasi Tempat Wisata di Indonesia/Dataset/paragraphs_en.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlaZrl8b4DMr"
      },
      "source": [
        "# START"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wue7vqHrNHUb"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain-community \\\n",
        "    langchain-chroma \\\n",
        "    langchain-huggingface \\\n",
        "    hf_xet langchain-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUPP3fC-48BM"
      },
      "outputs": [],
      "source": [
        "#=======================\n",
        "# Libraries\n",
        "#=======================\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "\n",
        "#=======================\n",
        "# Configuration\n",
        "#=======================\n",
        "CONFIG = {\n",
        "    \"data_path\": \"/content/drive/MyDrive/LLM/tourism-recommender/dataset/paragraphs_en.csv\",\n",
        "    \"chroma_path\": \"/content/drive/MyDrive/LLM/tourism-recommender/dataset/tourism_chroma_db\",\n",
        "    \"chunk_size\": 500,\n",
        "    \"chunk_overlap\": 200,\n",
        "    \"embedding_model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"embedding_model_kwargs\": {\"device\": \"cpu\"},\n",
        "    \"embedding_encode_kwargs\": {\"normalize_embeddings\": False},\n",
        "    \"RESET_DB\": False\n",
        "}\n",
        "\n",
        "#=======================\n",
        "# Reset DB\n",
        "#=======================\n",
        "if CONFIG[\"RESET_DB\"]:\n",
        "  shutil.rmtree(CONFIG[\"chroma_path\"], ignore_errors=True)\n",
        "\n",
        "#=======================\n",
        "# Data Loading\n",
        "#=======================\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path, header=None, names=['text'])\n",
        "    return '\\n'.join(df['text'].tolist())\n",
        "\n",
        "#=======================\n",
        "# Text Processing\n",
        "#=======================\n",
        "def split_text(text):\n",
        "    splitter = CharacterTextSplitter(\n",
        "        separator=\" \",\n",
        "        chunk_size=CONFIG['chunk_size'],\n",
        "        chunk_overlap=CONFIG['chunk_overlap']\n",
        "    )\n",
        "    documents = []\n",
        "    ids = []\n",
        "    chunks = splitter.split_text(text)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      chunk_id = f\"chunk-{i}\"\n",
        "      metadata = {\n",
        "        \"chunk_id\": chunk_id,\n",
        "        \"chunk_index\": i\n",
        "      }\n",
        "      documents.append(Document(page_content=chunk, metadata=metadata))\n",
        "      ids.append(chunk_id)\n",
        "    return documents, ids\n",
        "\n",
        "#=======================\n",
        "# Vector Store Setup\n",
        "#=======================\n",
        "def initialize_vector_store(docs, ids):\n",
        "  embeddings = HuggingFaceEmbeddings(\n",
        "      model_name=CONFIG[\"embedding_model_name\"],\n",
        "      model_kwargs=CONFIG[\"embedding_model_kwargs\"],\n",
        "      encode_kwargs=CONFIG[\"embedding_encode_kwargs\"],\n",
        "      show_progress=True\n",
        "  )\n",
        "\n",
        "  if os.path.exists(CONFIG['chroma_path']) and os.path.isdir(CONFIG['chroma_path']):\n",
        "    vector_store = Chroma(\n",
        "        persist_directory=CONFIG[\"chroma_path\"],\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    print(f\"Vector store loaded from {CONFIG['chroma_path']}\")\n",
        "  else:\n",
        "    vector_store = Chroma.from_documents(\n",
        "        ids=ids,\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=CONFIG[\"chroma_path\"]\n",
        "    )\n",
        "    print(f\"Vector store saved to {CONFIG['chroma_path']}\")\n",
        "\n",
        "  return vector_store.as_retriever(search_kwargs={ \"k\": 10 })\n",
        "\n",
        "def format_docs_with_metadata(docs):\n",
        "  formatted = []\n",
        "  for i, doc in enumerate(docs):\n",
        "    metadata = doc.metadata\n",
        "    data_id = metadata.get(\"chunk_id\", \"Unknown ID\")\n",
        "    data_idx = metadata.get(\"chunk_index\", \"Unknown Index\")\n",
        "    formatted.append(\n",
        "        f\"Document {i+1} (ID: {data_id}, Index: {data_idx}):\\n{doc.page_content}\"\n",
        "    )\n",
        "    # formatted.append(doc.page_content)\n",
        "  return \"\\n\\n\".join(formatted)\n",
        "\n",
        "#=======================\n",
        "# LLM Setup\n",
        "#=======================\n",
        "def initialize_llm(message: str):\n",
        "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "  model_name = \"Qwen/Qwen3-1.7B\"\n",
        "  # load the tokenizer and the model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype=\"auto\",\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "  # prepare the model input\n",
        "  messages = [{\"role\": \"user\", \"content\": message}]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True,\n",
        "      enable_thinking=False\n",
        "  )\n",
        "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "  # conduct text completion\n",
        "  generated_ids = model.generate(\n",
        "      **model_inputs,\n",
        "      max_new_tokens=32768\n",
        "  )\n",
        "  output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "  # parsing thinking content\n",
        "  try:\n",
        "      # rindex finding 151668 (</think>)\n",
        "      index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "  except ValueError:\n",
        "      index = 0\n",
        "  thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "  content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "  return content\n",
        "\n",
        "#=======================\n",
        "# RAG Pipeline\n",
        "#=======================\n",
        "def create_rag_pipeline(retriever):\n",
        "  system_template = (\n",
        "      \"\"\"\n",
        "      You are an expert travel assistant for Indonesia.\n",
        "      Use this context to answer questions:\n",
        "      {context}\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", system_template),\n",
        "      (\"human\", \"{question}\")\n",
        "  ])\n",
        "\n",
        "  retrieval_chain = (\n",
        "      {\n",
        "          \"context\": lambda x: format_docs_with_metadata(retriever.invoke(x[\"question\"])),\n",
        "          \"question\": lambda x: x[\"question\"]\n",
        "      }\n",
        "      | prompt\n",
        "      | (lambda x: x.messages[0].content + \"\\nQuestion: \" + x.messages[1].content)\n",
        "      | (lambda x: initialize_llm(x))\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  return retrieval_chain\n",
        "\n",
        "#=======================\n",
        "# Main Execution\n",
        "#=======================\n",
        "if __name__ == \"__main__\":\n",
        "  raw_text = load_data(CONFIG[\"data_path\"])\n",
        "  docs, ids = split_text(raw_text)\n",
        "  vector_store = initialize_vector_store(docs, ids)\n",
        "  qa_chain = create_rag_pipeline(vector_store)\n",
        "\n",
        "  print(\"Welcome to Indonesia Travel Assistant! Type 'exit' to end.\")\n",
        "  user_input = \"Bring me the best and cheapest place to visit in Surabaya related to history! Tell me the reason too\"\n",
        "  try:\n",
        "    result = qa_chain.invoke({ \"question\": user_input })\n",
        "    print(f\"\\n💡 Answer: {result}\")\n",
        "  except Exception as e:\n",
        "    print(f\"\\n🚨 Error: {str(e)}\")\n",
        "\n",
        "  # while True:\n",
        "  #   user_input = input(\"\\nYour question: \").strip()\n",
        "  #   if user_input.lower() in ['exit', 'quit']:\n",
        "  #     break\n",
        "\n",
        "  #   try:\n",
        "  #     result = qa_chain.invoke({ \"question\": user_input })\n",
        "  #     print(f\"\\n💡 Answer: {result}\")\n",
        "  #   except Exception as e:\n",
        "  #     print(f\"\\n🚨 Error: {str(e)}\")\n",
        "\n",
        "print(\"\\nThank you for using the travel assistant!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "168p8Tr-mgEA3cpDN9DMeVz1ClVYbjBiy",
      "authorship_tag": "ABX9TyPXs9PNTfwTgNUX4GX+8+lW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}